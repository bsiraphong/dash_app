{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash app running on http://127.0.0.1:8052/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-9b8f1f9f5134>:183: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-31-9b8f1f9f5134>:184: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-31-9b8f1f9f5134>:185: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "<ipython-input-31-9b8f1f9f5134>:186: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc\n",
    "import dash_core_components as dcc\n",
    "from dash import html\n",
    "import dash_html_components as html\n",
    "import plotly.express as px\n",
    "from wordcloud import WordCloud\n",
    "from googleapiclient.discovery import build\n",
    "from jupyter_dash import JupyterDash\n",
    "from dash.dependencies import Output, Input, State\n",
    "from dash.exceptions import PreventUpdate\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from functools import lru_cache\n",
    "from pythainlp.corpus.common import thai_stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = joblib.load('text_svm.pkl')\n",
    "vectorizer = joblib.load('vectorizer.pkl')\n",
    "\n",
    "# Set up the Dash app\n",
    "app = JupyterDash(__name__)\n",
    "\n",
    "app.layout = html.Div(\n",
    "    style={'text-align': 'center'},\n",
    "    children=[\n",
    "        html.H1(\"YouTube Comment Sentiment Analysis\"),\n",
    "        html.Div([\n",
    "            html.Label(\"Enter a YouTube video ID:\"),\n",
    "            dcc.Input(id=\"video-id-input\", type=\"text\"),\n",
    "            html.Button(\"Submit\", id=\"submit-button\"),\n",
    "        ]),\n",
    "        html.Div(\n",
    "            style={'text-align': 'center'},\n",
    "            children=\n",
    "            [html.Label(\"Choose a word cloud to display:\"),\n",
    "            dcc.Dropdown(\n",
    "                id='dropdown',\n",
    "                options=[\n",
    "                    {'label': 'All comments', 'value': 'all'},\n",
    "                    {'label': 'Positive comments only', 'value': 'positive'},\n",
    "                    {'label': 'Negative comments only', 'value': 'negative'}\n",
    "                ],\n",
    "                value='all'\n",
    "            )\n",
    "        ]),\n",
    "        html.H2(\"YouTube Comment Word Cloud\"),\n",
    "        html.Div(id=\"output-div\",\n",
    "        children=html.Div(style={'display': 'flex', 'align-items': 'center', 'justify-content': 'center'})\n",
    "            ),\n",
    "        html.Div(\n",
    "            style={'margin': 'auto'},\n",
    "             children=[\n",
    "                html.H3(\"Sentiment and Likes by Comment\"),\n",
    "                dcc.Graph(id='sunburst-chart', figure={})\n",
    "                ]),\n",
    "        html.H4('Time Series Plot'),\n",
    "        dcc.Graph(id='time-series-plot', figure={}), # new graph component\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=1024)\n",
    "def get_comments(video_id):\n",
    "    # Scrape the comments for the specified video\n",
    "    api_key = 'AIzaSyC5cwg9T2jF5GvVBCJIN-tcK7MMW1MnDS4'\n",
    "\n",
    "    # Replace VIDEO_ID with the ID of the video you want to scrape comments from\n",
    "    video_id = str(video_id)\n",
    "\n",
    "    # Create a YouTube service object\n",
    "    service = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "    data = []\n",
    "    max_results = 1000\n",
    "    next_page_token = ''\n",
    "\n",
    "    while True:\n",
    "        # Call the YouTube API to retrieve comments for the video\n",
    "        request = service.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            textFormat='plainText',\n",
    "            maxResults=max_results,\n",
    "            pageToken=next_page_token)\n",
    "        response = request.execute()\n",
    "\n",
    "        # Iterate over the response items\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            data.append({\n",
    "                'text': comment['textDisplay'],\n",
    "                'author': comment['authorDisplayName'],\n",
    "                'likes': comment['likeCount'],\n",
    "                'publishedAt': comment['publishedAt']})\n",
    "\n",
    "        # Check if there is a next page of results\n",
    "        if 'nextPageToken' in response:\n",
    "            next_page_token = response['nextPageToken']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Process the comments to extract the text and clean it up\n",
    "    stop_words = thai_stopwords()\n",
    "    df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "    # Classify the comments as positive or negative\n",
    "    X = vectorizer.transform(df['text'])\n",
    "    predictions = model.predict(X)\n",
    "    df['sentiment'] = predictions\n",
    "\n",
    "    # Create dataframes for positive and negative comments\n",
    "    positive_df = df.loc[df['sentiment'] == 'pos']\n",
    "    negative_df = df.loc[df['sentiment'] == 'neg']\n",
    "\n",
    "    return df, positive_df, negative_df\n",
    "\n",
    "#wordcloud\n",
    "@app.callback(\n",
    "    Output(\"output-div\", \"children\"),\n",
    "    [Input(\"submit-button\", \"n_clicks\"), Input(\"dropdown\", \"value\")],\n",
    "    [State(\"video-id-input\", \"value\")],\n",
    ")\n",
    "def update_output(n_clicks, dropdown_value, video_id):\n",
    "    if not n_clicks:\n",
    "        raise PreventUpdate\n",
    "    else:\n",
    "        df, positive_df, negative_df = get_comments(video_id)\n",
    "\n",
    "        # Generate word clouds\n",
    "        reg = r\"[ก-๙a-zA-Z']+\"\n",
    "        fp = \"THSarabunNew.ttf\"\n",
    "        all_wordcloud = WordCloud(background_color = 'white', max_words=1000, height = 500, width=800, font_path=fp, regexp=reg).generate(\" \".join(df['text']))\n",
    "        positive_wordcloud = WordCloud(background_color = 'white', max_words=1000, height = 500, width=800, font_path=fp, regexp=reg).generate(\" \".join(positive_df['text']))\n",
    "        negative_wordcloud = WordCloud(background_color = 'white', max_words=1000, height = 500, width=800, font_path=fp, regexp=reg).generate(\" \".join(negative_df['text']))\n",
    "\n",
    "        if dropdown_value == 'all':\n",
    "            return html.Img(src=all_wordcloud.to_image())\n",
    "        elif dropdown_value == 'positive':\n",
    "            return html.Img(src=positive_wordcloud.to_image())\n",
    "        elif dropdown_value == 'negative':\n",
    "            return html.Img(src=negative_wordcloud.to_image())\n",
    "#sunburst\n",
    "@app.callback(\n",
    "    Output(\"sunburst-chart\", \"figure\"),\n",
    "    [Input(\"submit-button\", \"n_clicks\"), Input(\"dropdown\", \"value\")],\n",
    "    [State(\"video-id-input\", \"value\")],\n",
    ")\n",
    "def update_sunburst_chart(n_clicks, dropdown_value, video_id):\n",
    "    if not n_clicks:\n",
    "        raise PreventUpdate\n",
    "    else:\n",
    "        df, positive_df, negative_df = get_comments(video_id)\n",
    "\n",
    "        if dropdown_value == 'all':\n",
    "            df_plot = df\n",
    "        elif dropdown_value == 'positive':\n",
    "            df_plot = positive_df\n",
    "        elif dropdown_value == 'negative':\n",
    "            df_plot = negative_df\n",
    "\n",
    "        # Create the sunburst chart\n",
    "        fig = px.sunburst(df_plot, path=['sentiment', 'likes', 'text'], values='likes', height = 1000, width=1500,color='sentiment',\n",
    "        color_discrete_map={'pos':'rgb(136,204,238)', 'neg':'rgb(251,128,114)'})\n",
    "        \n",
    "        return fig\n",
    "#line plot\n",
    "@app.callback(\n",
    "    Output('time-series-plot', 'figure'),\n",
    "    [Input('video-id-input', 'value')]\n",
    ")\n",
    "def update_time_series(video_id):\n",
    "    if video_id is None:\n",
    "        raise PreventUpdate\n",
    "    else:\n",
    "        df, positive_df, negative_df = get_comments(video_id)\n",
    "        positive_df['publishedAt'] = pd.to_datetime(positive_df['publishedAt'])\n",
    "        negative_df['publishedAt'] = pd.to_datetime(negative_df['publishedAt'])\n",
    "        positive_df['month'] = positive_df['publishedAt'].apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "        negative_df['month'] = negative_df['publishedAt'].apply(lambda x: x.strftime(\"%Y-%m\"))\n",
    "        positive_monthly_summary = positive_df.groupby('month').agg({'sentiment': 'count'})\n",
    "        negative_monthly_summary = negative_df.groupby('month').agg({'sentiment': 'count'})\n",
    "        positive_monthly_summary.columns = ['positive_comment_count']\n",
    "        negative_monthly_summary.columns = ['negative_comment_count']\n",
    "        monthly_summary = pd.merge(positive_monthly_summary, negative_monthly_summary, left_index=True, right_index=True)\n",
    "        monthly_summary['positive_cumsum'] = monthly_summary['positive_comment_count'].cumsum()\n",
    "        monthly_summary['negative_cumsum'] = monthly_summary['negative_comment_count'].cumsum()\n",
    "\n",
    "        # create the plotly figure\n",
    "        fig = px.line(monthly_summary, y=['positive_cumsum', 'negative_cumsum'])\n",
    "        fig.update_layout(xaxis_title=\"Month\",yaxis_title=\"comment count\")\n",
    "        \n",
    "\n",
    "        return fig\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True,port=8052)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04f920ff97d607c3ad4933edb24df31d6c0eca1c1e83189b75f0dd2ec5e12310"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
